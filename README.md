# API de Web Scraping Modular

API RESTful para realizar scraping de mÃºltiples pÃ¡ginas web de forma modular y extensible.

## ğŸš€ CaracterÃ­sticas

- **Arquitectura modular**: FÃ¡cil agregar nuevos scrapers para diferentes pÃ¡ginas
- **API RESTful**: Endpoints documentados con FastAPI
- **GestiÃ³n centralizada**: Manejador general que enruta a scrapers especÃ­ficos
- **Extensible**: Clase base para crear nuevos scrapers rÃ¡pidamente
- **Ordenamiento inteligente**: Resultados ordenados por preferencia (720p > 1080p > 4K > SD)
- **Dos versiones de scrapers**: Completo (con alternativas) y RÃ¡pido (solo mejores opciones)
- **Optimizado para performance**: VersiÃ³n rÃ¡pida hasta 73% mÃ¡s veloz

## ğŸ“ Estructura del Proyecto

```
scrap_api/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes.py          # Endpoints de la API
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py    # Clase base abstracta
â”‚   â”œâ”€â”€ example_scraper.py # Ejemplo de scraper
â”‚   â”œâ”€â”€ dontorrent_scraper.py # Scraper de torrents (completo)
â”‚   â”œâ”€â”€ dontorrent_fast_scraper.py # Scraper optimizado (rÃ¡pido)
â”‚   â””â”€â”€ manager.py         # Manejador general
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py         # Modelos Pydantic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py         # Utilidades comunes
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ reference/         # CÃ³digo original de referencia
â”œâ”€â”€ main.py                # Punto de entrada
â”œâ”€â”€ config.py              # ConfiguraciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaciÃ³n

1. Crear entorno virtual:
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

2. Instalar dependencias:
```bash
pip install -r requirements.txt
```

3. Configurar variables de entorno (opcional):
```bash
cp .env.example .env
# Editar .env con tus configuraciones
```

## ğŸƒ Uso

1. Iniciar el servidor:
```bash
python main.py
```

O con uvicorn directamente:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

2. Acceder a la documentaciÃ³n interactiva:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ“ Endpoints

### GET /
InformaciÃ³n general de la API

### POST /scrape
Realizar scraping de una pÃ¡gina

**Request Body:**
```json
{
  "site": "example",
  "criteria": {
    "url": "https://example.com",
    "selector": "div.content"
  }
}
```

**Response:**
```json
{
  "site": "example",
  "data": [...],
  "timestamp": "2025-11-25T10:30:00Z",
  "status": "success"
}
```

### GET /sites
Listar todos los scrapers disponibles

## ğŸ¯ Scrapers Disponibles

### 1. Dontorrent (Completo) - `dontorrent`
- **Uso**: Interfaces interactivas, cuando necesitas ver todas las opciones
- **Devuelve**: Mejores torrents + alternativas disponibles
- **Performance**: ~5-10 segundos para una temporada
- **DocumentaciÃ³n**: [DONTORRENT_GUIDE.md](./DONTORRENT_GUIDE.md)

### 2. Dontorrent Fast - `dontorrent-fast` âš¡
- **Uso**: Automatizaciones, scripts, descargas masivas
- **Devuelve**: Solo los mejores torrents (sin alternativas)
- **Performance**: ~1-3 segundos para una temporada (73% mÃ¡s rÃ¡pido)
- **DocumentaciÃ³n**: [DONTORRENT_GUIDE.md](./DONTORRENT_GUIDE.md)

**Ejemplo de uso:**

```bash
# VersiÃ³n completa (con alternativas)
curl -X POST http://localhost:7002/scrape -H "Content-Type: application/json" -d '{
  "site": "dontorrent",
  "criteria": {
    "serie_name": "Breaking Bad",
    "start_episode": "S01E01",
    "end_episode": "S01E10",
    "base_url": "https://dontorrent.monster"
  }
}'

# VersiÃ³n rÃ¡pida (solo mejores) âš¡
curl -X POST http://localhost:7002/scrape -H "Content-Type: application/json" -d '{
  "site": "dontorrent-fast",
  "criteria": {
    "serie_name": "Breaking Bad",
    "start_episode": "S01E01",
    "end_episode": "S01E10",
    "base_url": "https://dontorrent.monster"
  }
}'
```

## ğŸ”§ Agregar un Nuevo Scraper

1. Crear un nuevo archivo en `scrapers/`:

```python
from scrapers.base_scraper import BaseScraper
from typing import Dict, Any

class MiNuevoScraper(BaseScraper):
    def __init__(self):
        super().__init__(name="mi_sitio")
    
    def scrape(self, criteria: Dict[str, Any]) -> Dict[str, Any]:
        url = criteria.get("url")
        # Tu lÃ³gica de scraping aquÃ­
        soup = self.get_soup(url)
        data = soup.find_all("div", class_="item")
        
        return {
            "items": [item.text for item in data],
            "count": len(data)
        }
    
    def validate_criteria(self, criteria: Dict[str, Any]) -> bool:
        return "url" in criteria
```

2. Registrar el scraper en `scrapers/manager.py`:

```python
from scrapers.mi_nuevo_scraper import MiNuevoScraper

# En el __init__ de ScraperManager:
self.register_scraper("mi_sitio", MiNuevoScraper())
```

## ğŸ³ Docker (Opcional)

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ“„ Licencia

MIT
